# Nano-GPT: Construindo um Transformer do Zero

## ü§ñ O que √© este projeto? (Um breve resumo)

Este projeto √© um "c√©rebro" de Intelig√™ncia Artificial que eu constru√≠ do zero para aprender a escrever.

Pense nele como um "Estudante Rob√¥":
1.  **A Mat√©ria:** Eu dei a ele um √∫nico livro para estudar (*A Story of the Golden Age*).
2.  **O Estudo:** O rob√¥ leu o livro letra por letra, milhares de vezes, at√© aprender a *prever* qual √© a pr√≥xima letra mais prov√°vel em qualquer frase.
3.  **A Prova:** No final, ele ficou t√£o bom em adivinhar os padr√µes que agora consegue "escrever" seus pr√≥prios par√°grafos que, embora n√£o fa√ßam sentido completo, se *parecem* muito com o estilo do livro original.

A "Jornada" (Beb√™, Adolescente, Adulto) foi o meu experimento cient√≠fico para descobrir o "tamanho" de c√©rebro ideal para esse rob√¥ aprender a mat√©ria sem s√≥ "decorar" o livro.

---

## üöÄ Arquitetura e Features

Este projeto √© uma implementa√ß√£o de um modelo de linguagem Transformer (estilo GPT) em PyTorch, constru√≠do do zero para fins de estudo. O modelo √© treinado em n√≠vel de caractere para gerar texto baseado em um corpus de entrada.

O foco principal deste reposit√≥rio n√£o √© apenas o c√≥digo final, mas a **jornada iterativa de engenharia** para construir um modelo que aprende de forma eficaz, mesmo em um ambiente de CPU limitado.

Este modelo √© um "Transformer Decoder-Only" (a mesma arquitetura do GPT) e inclui:

* **Tokeniza√ß√£o em N√≠vel de Caractere**: O vocabul√°rio √© composto por todos os caracteres √∫nicos do texto de entrada.
* **Embeddings de Token e Posi√ß√£o**: Para dar ao modelo o significado dos caracteres e seu senso de ordem.
* **Blocos Transformer**: O cora√ß√£o do modelo, empilhados `n_layer` vezes.
* **Multi-Head Self-Attention**: O mecanismo que permite ao modelo "prestar aten√ß√£o" a diferentes partes do contexto para prever o pr√≥ximo caractere.
* **Rede Feed-Forward**: Uma camada de "reflex√£o" para cada token processar a informa√ß√£o da aten√ß√£o.
* **Conex√µes Residuais e Layer Normalization**: Essencial para estabilizar o treinamento em redes profundas.
* **Gera√ß√£o de Texto Autoregressiva**: O modelo usa sua pr√≥pria sa√≠da como entrada para gerar texto novo.

---

## üõ†Ô∏è Como Executar

1.  **Clone o reposit√≥rio:**
    ```bash
    git clone [https://github.com/Arthurml97/nano-transformer.git](https://github.com/Arthurml97/nano-transformer.git)
    cd nano-transformer
    ```

2.  **Crie um ambiente virtual e instale as depend√™ncias:**
    ```bash
    python -m venv venv
    source venv/bin/activate  # No Windows: .\venv\Scripts\activate
    pip install -r requirements.txt
    ```

3.  **Prepare os dados:**
    Coloque o seu arquivo de texto de treinamento na raiz do projeto com o nome `input.txt`.

4.  **Treine o modelo:**
    O script `nano.py` est√° configurado com hiperpar√¢metros otimizados para CPU.
    ```bash
    python nano.py
    ```

---

## üî¨ A Jornada: Do Overfitting ao Aprendizado

Este projeto foi uma experi√™ncia pr√°tica sobre o equil√≠brio entre o tamanho do modelo, o tamanho dos dados e as limita√ß√µes de hardware.

### Ponto de Partida: O Dataset M√≠nimo

Inicialmente, o projeto come√ßou com um dataset muito pequeno (um breve resumo da lore de World of Warcraft). Nos est√°gios iniciais (quando o modelo era mais simples), ele funcionou.

No entanto, √† medida que a arquitetura evoluiu para um Transformer completo, o modelo imediatamente "decorou" (overfit) esse dataset min√∫sculo em poucas itera√ß√µes. Ele se tornou incapaz de aprender qualquer regra generaliz√°vel do idioma. Isso provou que, para um modelo mais complexo, um dataset maior n√£o era opcional ‚Äî era obrigat√≥rio. Foi feita ent√£o a troca para um corpus muito maior: o livro *A Story of the Golden Age*.

### O Desafio da CPU: O Teste de 10.7M de Par√¢metros

Antes de otimizar o modelo para a CPU, foi realizado um teste crucial: "O que acontece se o modelo otimizado para GPU (10.7M de par√¢metros) for treinado na minha CPU (Ryzen 5 5600)?"

O resultado foi, como esperado, **improdutivo**. O treinamento levou **mais de 12 horas** para gerar uma resposta b√°sica e com overfitting severo.

Esse teste provou que uma abordagem "for√ßa bruta" n√£o era vi√°vel. A solu√ß√£o seria come√ßar do zero, com um modelo pequeno o suficiente para a CPU, e otimiz√°-lo iterativamente.

### A Estrat√©gia: For√ßando a Generaliza√ß√£o (Bottom-Up)

A estrat√©gia mudou para: "Qual √© o modelo *mais inteligente* que eu consigo treinar na minha CPU *em um tempo razo√°vel*?"

O processo foi feito em tr√™s est√°gios, aumentando o "c√©rebro" do modelo a cada passo:

#### 1. "Beb√™ Transformer" (0.2M de par√¢metros)

* **Config:** `n_embd=64`, `n_head=4`, `n_layer=4`
* **Resultado:** `val loss ~2.04`. O modelo gerou um "Ingl√™s-Fantasma"‚Äîtexto que tinha a *forma* do ingl√™s (espa√ßos, pontua√ß√£o, finais como "ing"), mas sem palavras reais. **Sucesso!** A generaliza√ß√£o estava acontecendo.

#### 2. "Adolescente Transformer" (0.8M de par√¢metros)

* **Config:** `n_embd=128`, `n_head=4`, `n_layer=4`
* **Resultado:** `val loss ~1.90`. O modelo, com mais capacidade, come√ßou a gerar palavras reais do livro, como "Hellas", "Neleus" e "Iphig's".

#### 3. "Adulto Transformer" (1.2M de par√¢metros)

* **Config:** `n_embd=128`, `n_head=6`, `n_layer=6`, `dropout=0.2`
* **Resultado:** `val loss` m√≠nimo de **1.88**. Este foi o modelo mais inteligente. Ele atingiu seu pico de aprendizado por volta de `step 3500` e depois come√ßou a overfitar.

| Modelo | Par√¢metros | Melhor Val Loss | Texto Gerado (Exemplo) |
| :--- | :--- | :--- | :--- |
| Bab√™ | 0.2M | 2.0423 | `...intoring Ithaca. he made Gram unthis...` |
| Adolescente | 0.8M | 1.9059 | `...were Neleus to the olders of Mount Iphig‚Äôs...` |
| Adulto | 1.2M | **1.8842** | `...said Phemius," "and away bless wrookly upon the dutyings...` |

### üí° Conclus√£o da Jornada

Este projeto foi uma demonstra√ß√£o pr√°tica de que:
1.  **Hardware Limita o Design**: A falha no teste de 12 horas na CPU for√ßou uma abordagem de design de modelo "de baixo para cima" (bottom-up), focada em efici√™ncia.
2.  **O Overfitting √© Vis√≠vel**: Ao monitorar o `val loss`, foi poss√≠vel identificar *exatamente* quando o modelo parou de aprender e come√ßou a decorar (por volta de `step 3500-4000` nos modelos maiores).
3.  **O N√≠vel de Caractere Aprende Estrutura**: Mesmo sem saber o que √© uma "palavra", o Transformer aprendeu regras de sintaxe, pontua√ß√£o e forma√ß√£o de palavras do texto de entrada.

---
## üìä Google Colab e 3 Million Dataset

Os experimentos anteriores provaram que o hardware (CPU) e o conjunto de dados (um √∫nico livro) eram os gargalos.

Esta branch leva o projeto √† sua conclus√£o l√≥gica:
1.  **Hardware:** O treinamento ser√° movido para o Google Colab para usar uma GPU T4.
2.  **Modelo:** Vou usar o "super-c√©rebro" de 14.5M de par√¢metros (ativando os hiperpar√¢metros de CUDA).
3.  **Dados:** O `input.txt` ser√° expandido para uma Obra de Tolkien.

O objetivo √©, finalmente, treinar um modelo onde o `val loss` *diminua* de forma est√°vel, provando que a arquitetura BPE √© vi√°vel quando recebe os recursos adequados.

## üìú Cr√©ditos

Este c√≥digo foi desenvolvido como parte de um estudo aprofundado do reposit√≥rio [nanoGPT](https://github.com/karpathy/nanoGPT) de Andrej Karpathy, adaptado para um ambiente de CPU e focado na an√°lise iterativa de hiperpar√¢metros.
